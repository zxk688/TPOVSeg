import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from typing import Dict, List, Optional, Tuple

class MultiPositionGradCAM:
    """
    åœ¨CAT-Segæ¨¡å‹çš„å¤šä¸ªä½ç½®ç”Ÿæˆåˆ†ç±»åˆ«CAM
    """
    
    def __init__(self, model, target_positions: List[str] = None):
        """
        Args:
            model: CAT-Segæ¨¡å‹
            target_positions: è¦åˆ†æçš„ä½ç½®åˆ—è¡¨ ['corr_embed', 'channel_attn', 'spatial_agg']
        """
        self.model = model
        self.target_positions = target_positions or ['corr_embed', 'channel_attn', 'spatial_agg']
        
        # å­˜å‚¨æ¿€æ´»å€¼å’Œæ¢¯åº¦çš„å­—å…¸
        self.activations = {}
        self.gradients = {}
        
        # æ³¨å†Œhook
        self.hooks = []
        self._register_hooks()
    
    def _register_hooks(self):
        """æ³¨å†Œå‰å‘å’Œåå‘hook"""
        
        # Position 1: corr_embedä¹‹å
        if 'corr_embed' in self.target_positions:
            self._register_position_hook('corr_embed', self._find_corr_embed_layer())
        
        # Position 2: channel attentionä¹‹å
        if 'channel_attn' in self.target_positions:
            self._register_position_hook('channel_attn', self._find_channel_attn_layer())
        
        # Position 3: spatial aggregationä¹‹å  
        if 'spatial_agg' in self.target_positions:
            self._register_position_hook('spatial_agg', self._find_spatial_agg_layer())
    
    def _find_corr_embed_layer(self):
        """æ‰¾åˆ°corr_embedåçš„å±‚ï¼Œè¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“æ¨¡å‹ç»“æ„è°ƒæ•´"""
        # ç”±äºcorr_embedæ˜¯å‡½æ•°è°ƒç”¨ï¼Œæˆ‘ä»¬éœ€è¦hookåˆ°conv1å±‚
        return self.model.conv1
    
    def _find_channel_attn_layer(self):
        """æ‰¾åˆ°é€šé“æ³¨æ„åŠ›å±‚"""
        # å‡è®¾æ˜¯ç¬¬ä¸€ä¸ªaggregator layerçš„channel_attention
        return self.model.layers[0].channel_attention
    
    def _find_spatial_agg_layer(self):
        """æ‰¾åˆ°ç©ºé—´èšåˆå±‚"""
        # å‡è®¾æ˜¯ç¬¬ä¸€ä¸ªaggregator layerçš„swin_block
        return self.model.layers[0].swin_block
    
    def _register_position_hook(self, position_name: str, target_layer):
        """ä¸ºç‰¹å®šä½ç½®æ³¨å†Œhook"""
        
        def forward_hook(module, input, output):
            # å­˜å‚¨æ¿€æ´»å€¼ï¼Œéœ€è¦æ ¹æ®è¾“å‡ºæ ¼å¼è°ƒæ•´
            if isinstance(output, tuple):
                self.activations[position_name] = output[0]
            else:
                self.activations[position_name] = output
        
        def backward_hook(module, grad_input, grad_output):
            # å­˜å‚¨æ¢¯åº¦
            if isinstance(grad_output, tuple):
                self.gradients[position_name] = grad_output[0]
            else:
                self.gradients[position_name] = grad_output
        
        # æ³¨å†Œhook
        fhook = target_layer.register_forward_hook(forward_hook)
        bhook = target_layer.register_backward_hook(backward_hook)
        self.hooks.extend([fhook, bhook])
    
    def generate_multi_position_cam(
        self, 
        input_image, 
        text_features, 
        appearance_guidance,
        target_class: int,
        batch_idx: int = 0
    ) -> Dict[str, np.ndarray]:
        """
        ç”Ÿæˆå¤šä¸ªä½ç½®çš„CAM
        
        Args:
            input_image: è¾“å…¥å›¾åƒ
            text_features: æ–‡æœ¬ç‰¹å¾
            appearance_guidance: å¤–è§‚æŒ‡å¯¼
            target_class: ç›®æ ‡ç±»åˆ«ç´¢å¼•
            batch_idx: æ‰¹æ¬¡ç´¢å¼•
            
        Returns:
            å„ä½ç½®çš„CAMå­—å…¸
        """
        
        # ç¡®ä¿è¾“å…¥éœ€è¦æ¢¯åº¦
        input_image.requires_grad_(True)
        
        # å‰å‘ä¼ æ’­
        self.model.eval()
        output = self.model(input_image, text_features, appearance_guidance)
        
        # è®¡ç®—ç›®æ ‡åˆ†æ•°ï¼ˆæ ¹æ®å…·ä½“è¾“å‡ºæ ¼å¼è°ƒæ•´ï¼‰
        if len(output.shape) == 4:  # [B, T, H, W]
            target_score = output[batch_idx, target_class].sum()
        else:
            target_score = output[batch_idx, target_class]
        
        # åå‘ä¼ æ’­
        self.model.zero_grad()
        target_score.backward()
        
        # ä¸ºæ¯ä¸ªä½ç½®è®¡ç®—CAM
        cams = {}
        for position in self.target_positions:
            if position in self.activations and position in self.gradients:
                cam = self._compute_single_position_cam(
                    position, target_class, batch_idx
                )
                cams[position] = cam
        
        return cams
    
    def _compute_single_position_cam(
        self, 
        position: str, 
        target_class: int, 
        batch_idx: int
    ) -> np.ndarray:
        """
        è®¡ç®—å•ä¸ªä½ç½®çš„CAM
        """
        
        activations = self.activations[position]
        gradients = self.gradients[position]
        
        # æ ¹æ®å¼ é‡ç»´åº¦å¤„ç†
        if len(activations.shape) == 5:  # [B, C, T, H, W]
            # æå–ç‰¹å®šç±»åˆ«
            act = activations[batch_idx, :, target_class, :, :]  # [C, H, W]
            grad = gradients[batch_idx, :, target_class, :, :]   # [C, H, W]
        elif len(activations.shape) == 4:  # [B, C, H, W]
            act = activations[batch_idx]
            grad = gradients[batch_idx] 
        else:
            raise ValueError(f"Unsupported activation shape: {activations.shape}")
        
        # è®¡ç®—æƒé‡
        weights = torch.mean(grad, dim=(1, 2), keepdim=True)  # [C, 1, 1]
        
        # åŠ æƒç»„åˆ
        cam = torch.sum(weights * act, dim=0)  # [H, W]
        
        # ReLUå’Œå½’ä¸€åŒ–
        cam = F.relu(cam)
        if cam.max() > 0:
            cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)
        
        return cam.detach().cpu().numpy()
    
    def visualize_multi_position_cam(
        self, 
        original_image: np.ndarray,
        cams: Dict[str, np.ndarray],
        class_name: str = None,
        save_path: str = None
    ):
        """
        å¯è§†åŒ–å¤šä¸ªä½ç½®çš„CAMç»“æœ
        """
        
        num_positions = len(cams)
        fig, axes = plt.subplots(1, num_positions + 1, figsize=(4 * (num_positions + 1), 4))
        
        # æ˜¾ç¤ºåŸå›¾
        axes[0].imshow(original_image)
        axes[0].set_title("Original Image")
        axes[0].axis('off')
        
        # æ˜¾ç¤ºå„ä½ç½®çš„CAM
        position_names = {
            'corr_embed': 'Raw Correlation',
            'channel_attn': 'Channel Enhanced', 
            'spatial_agg': 'Spatially Refined'
        }
        
        for idx, (position, cam) in enumerate(cams.items()):
            # è°ƒæ•´CAMå¤§å°åˆ°åŸå›¾å°ºå¯¸
            h, w = original_image.shape[:2]
            cam_resized = self._resize_cam(cam, (h, w))
            
            # ç”Ÿæˆçƒ­åŠ›å›¾
            heatmap = self._apply_colormap(cam_resized)
            
            # å åŠ æ˜¾ç¤º
            overlay = self._overlay_cam(original_image, heatmap, alpha=0.4)
            
            axes[idx + 1].imshow(overlay)
            title = position_names.get(position, position)
            if class_name:
                title += f"\n({class_name})"
            axes[idx + 1].set_title(title)
            axes[idx + 1].axis('off')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        
        plt.show()
    
    def _resize_cam(self, cam: np.ndarray, target_size: Tuple[int, int]) -> np.ndarray:
        """è°ƒæ•´CAMå¤§å°"""
        import cv2
        return cv2.resize(cam, target_size)
    
    def _apply_colormap(self, cam: np.ndarray) -> np.ndarray:
        """åº”ç”¨é¢œè‰²æ˜ å°„"""
        import cv2
        heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)
        return cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)
    
    def _overlay_cam(self, image: np.ndarray, heatmap: np.ndarray, alpha: float = 0.4) -> np.ndarray:
        """å åŠ CAMå’ŒåŸå›¾"""
        return heatmap * alpha + image * (1 - alpha)
    
    def compare_positions(
        self,
        input_image,
        text_features, 
        appearance_guidance,
        target_class: int,
        class_name: str = None
    ) -> Dict[str, float]:
        """
        æ¯”è¾ƒä¸åŒä½ç½®CAMçš„è´¨é‡æŒ‡æ ‡
        """
        
        cams = self.generate_multi_position_cam(
            input_image, text_features, appearance_guidance, target_class
        )
        
        metrics = {}
        for position, cam in cams.items():
            # è®¡ç®—CAMè´¨é‡æŒ‡æ ‡
            metrics[position] = {
                'peak_value': float(cam.max()),
                'coverage': float((cam > 0.5).sum() / cam.size),
                'concentration': float(np.std(cam)),
                'center_mass': self._calculate_center_of_mass(cam)
            }
        
        return metrics
    
    def _calculate_center_of_mass(self, cam: np.ndarray) -> Tuple[float, float]:
        """è®¡ç®—CAMçš„è´¨å¿ƒ"""
        from scipy.ndimage import center_of_mass
        return center_of_mass(cam)
    
    def cleanup(self):
        """æ¸…ç†hook"""
        for hook in self.hooks:
            hook.remove()
        self.hooks.clear()
        self.activations.clear()
        self.gradients.clear()

# ä½¿ç”¨ç¤ºä¾‹
class CATSegGradCAMAnalyzer:
    """CAT-Segæ¨¡å‹çš„å®Œæ•´CAMåˆ†æå·¥å…·"""
    
    def __init__(self, model):
        self.model = model
        self.gradcam = MultiPositionGradCAM(model)
    
    def analyze_class_attention(
        self,
        input_image,
        text_features,
        appearance_guidance, 
        target_classes: List[int],
        class_names: List[str] = None
    ):
        """
        åˆ†æå¤šä¸ªç±»åˆ«åœ¨ä¸åŒä½ç½®çš„æ³¨æ„åŠ›æ¨¡å¼
        """
        
        results = {}
        
        for i, class_idx in enumerate(target_classes):
            class_name = class_names[i] if class_names else f"Class_{class_idx}"
            
            # ç”Ÿæˆè¯¥ç±»åˆ«çš„CAM
            cams = self.gradcam.generate_multi_position_cam(
                input_image, text_features, appearance_guidance, class_idx
            )
            
            # è®¡ç®—è´¨é‡æŒ‡æ ‡
            metrics = self.gradcam.compare_positions(
                input_image, text_features, appearance_guidance, class_idx, class_name
            )
            
            results[class_name] = {
                'cams': cams,
                'metrics': metrics
            }
        
        return results
    
    def generate_comparison_report(self, analysis_results: Dict):
        """ç”Ÿæˆå¯¹æ¯”åˆ†ææŠ¥å‘Š"""
        
        print("=" * 60)
        print("CAT-Seg Multi-Position CAM Analysis Report")
        print("=" * 60)
        
        for class_name, result in analysis_results.items():
            print(f"\nğŸ“‹ {class_name}:")
            print("-" * 40)
            
            metrics = result['metrics']
            for position, metric in metrics.items():
                print(f"  {position:15s}: Peak={metric['peak_value']:.3f}, "
                      f"Coverage={metric['coverage']:.3f}, "
                      f"Concentration={metric['concentration']:.3f}")
        
        # æ‰¾å‡ºæœ€ä½³ä½ç½®
        print(f"\nğŸ¯ æ¨èåˆ†æä½ç½®:")
        print("-" * 40)
        
        position_scores = {}
        for class_name, result in analysis_results.items():
            for position, metric in result['metrics'].items():
                if position not in position_scores:
                    position_scores[position] = []
                # ç»¼åˆè¯„åˆ†ï¼ˆå¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´æƒé‡ï¼‰
                score = metric['peak_value'] * 0.4 + metric['concentration'] * 0.6
                position_scores[position].append(score)
        
        avg_scores = {pos: np.mean(scores) for pos, scores in position_scores.items()}
        best_position = max(avg_scores.keys(), key=lambda x: avg_scores[x])
        
        print(f"  æœ€ä½³ä½ç½®: {best_position} (å¹³å‡å¾—åˆ†: {avg_scores[best_position]:.3f})")
        
        return best_position

# å…·ä½“ä½¿ç”¨æ–¹æ³•
def usage_example():
    """ä½¿ç”¨ç¤ºä¾‹"""
    
    # 1. åˆ›å»ºåˆ†æå™¨
    analyzer = CATSegGradCAMAnalyzer(model)
    
    # 2. åˆ†æå¤šä¸ªç±»åˆ«
    target_classes = [0, 2, 4]  # è¦åˆ†æçš„ç±»åˆ«
    class_names = ["chair", "table", "sofa"]  # å¯¹åº”çš„ç±»åˆ«å
    
    results = analyzer.analyze_class_attention(
        input_image, text_features, appearance_guidance,
        target_classes, class_names
    )
    
    # 3. ç”ŸæˆæŠ¥å‘Š
    best_position = analyzer.generate_comparison_report(results)
    
    # 4. å¯è§†åŒ–æœ€æœ‰è¶£çš„ç»“æœ
    for class_name, result in results.items():
        analyzer.gradcam.visualize_multi_position_cam(
            original_image, result['cams'], class_name,
            save_path=f"cam_analysis_{class_name}.png"
        )
    
    # 5. æ¸…ç†
    analyzer.gradcam.cleanup()

if __name__ == "__main__":
    usage_example()









"""

å¥½çš„ï¼Œé‚£ä¹ˆæˆ‘ç°åœ¨æƒ³è¦åœ¨è¿™ä¸‰ä¸ªåœ°æ–¹å»åšåˆ†ç±»åˆ«çš„camï¼š
1.Aggregatorçš„forwardä¸­ï¼Œcorr_embed = self.corr_embed(corr)è¿™ä¸€è¡Œä»£ç ä¹‹åï¼Œè¿™ä¸ªæ˜¯ç›¸ä¼¼æ€§è®¡ç®—ä¹‹åå¾—åˆ°çš„äº”ç»´åº¦å¼ é‡ï¼ŒB,C,T,H,W
2.AggregatorLayerçš„forwardä¸­ï¼Œ        
x = self.channel_attention(x, text_guidance)        
x = self.swin_block(x, appearance_guidance)
è¿™ä¸¤ä¸ªä¹‹åã€‚è¿™ä¸¤ä¸ªåˆ†åˆ«æ˜¯é€šé“ç±»åˆ«èšåˆå™¨å’Œç©ºé—´èšåˆå™¨ã€‚
æˆ–è€…å«åšcost sliceï¼Ÿæˆ‘ä¸çŸ¥é“å’Œcamæœ‰æ²¡æœ‰åŒºåˆ«

"""
